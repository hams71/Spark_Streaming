{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode,split\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, LongType, DoubleType, FloatType, DataType, DateType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/06 11:39:45 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.197.132 instead (on interface ens33)\n",
      "22/09/06 11:39:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/09/06 11:39:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/09/06 11:39:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/09/06 11:39:47 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/09/06 11:39:47 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/09/06 11:39:47 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/09/06 11:39:47 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"Task_1\")\n",
    " .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(analytics,StructType(List(StructField(clicks,IntegerType,true),StructField(impressions,IntegerType,true))),true),StructField(datetime,TimestampType,true),StructField(sales,StructType(List(StructField(quantity,IntegerType,true),StructField(total_price,DoubleType,true))),true)))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Defining the schema and the datatypes of the columns\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sc = StructType([\n",
    "        StructField('analytics', StructType([\n",
    "             StructField('clicks', IntegerType(), True),\n",
    "             StructField('impressions', IntegerType(), True)\n",
    "             ])),\n",
    "         StructField('datetime', TimestampType(), True),\n",
    "         StructField('sales', StructType([\n",
    "             StructField('quantity', IntegerType(), True),\n",
    "             StructField('total_price', DoubleType(), True)\n",
    "             ]))\n",
    "         ])\n",
    "\n",
    "print(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"/home/ubuntu/Documents/Spark_Streaming/python_file/data/json_files\"\n",
    "csv_file_path = \"/home/ubuntu/Documents/Spark_Streaming/python_file/data/csv_files/\"\n",
    "csv_checkpoint_path = \"/home/ubuntu/Documents/Spark_Streaming/python_file/data/checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using the readStream as have to read data in stream from the json_files folder\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "json_df = spark.readStream.option(\"maxFilesPerTrigger\",5).schema(sc).format('json').option('header','true').load(json_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If its value is true then stream reading has started and we can perform transformation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "json_df.isStreaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "From nested to flat and dropping and remaining some of the columns\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = json_df\n",
    "\n",
    "\n",
    "df = df.withColumn('clicks',df.analytics.clicks)\n",
    "\n",
    "df = df.withColumn('impressions',df.analytics.impressions)\n",
    "\n",
    "df = df.withColumn('quantity',df.sales.quantity)\n",
    "df = df.withColumn('total_price',df.sales.total_price)\n",
    "\n",
    "df = df.drop(\"analytics\",\"sales\")\n",
    "\n",
    "#f_df = df.select('datetime','clicks','impressions','quantity','total_price')\n",
    "\n",
    "#f_df.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/06 11:40:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f2ff449ecd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Writing the data in stream as it is coming to up in CSV format.\n",
    "\n",
    "3 modes to write in append(only new data), complete(old data and new coming), update(updating the rows)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "final_df = df\n",
    "\n",
    "final_df.writeStream.outputMode(\"append\").format(\"csv\").option(\"path\", csv_file_path).option(\"checkpointLocation\", csv_checkpoint_path).option(\"header\", \"true\").start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
