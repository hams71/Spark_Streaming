{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode,split\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, LongType, DoubleType, FloatType, DataType, DateType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/10 12:33:33 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.197.132 instead (on interface ens33)\n",
      "22/08/10 12:33:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/10 12:33:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"Python\")\n",
    " .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(analytics,StructType(List(StructField(clicks,IntegerType,true),StructField(impressions,IntegerType,true))),true),StructField(datetime,TimestampType,true),StructField(sales,StructType(List(StructField(quantity,IntegerType,true),StructField(total_price,DoubleType,true))),true)))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sc = StructType([\n",
    "        StructField('analytics', StructType([\n",
    "             StructField('clicks', IntegerType(), True),\n",
    "             StructField('impressions', IntegerType(), True)\n",
    "             ])),\n",
    "         StructField('datetime', TimestampType(), True),\n",
    "         StructField('sales', StructType([\n",
    "             StructField('quantity', IntegerType(), True),\n",
    "             StructField('total_price', DoubleType(), True)\n",
    "             ]))\n",
    "         ])\n",
    "\n",
    "print(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('json').schema(sc).load('/home/ubuntu/Spark/Spark_Work/python_file/data/json_files/file_0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+------+-----------+--------+-----------------+\n",
      "|analytics|            datetime|               sales|clicks|impressions|quantity|      total_price|\n",
      "+---------+--------------------+--------------------+------+-----------+--------+-----------------+\n",
      "|  {6, 13}|2022-08-07T04:21:...|            {5, 6.4}|     6|         13|       5|              6.4|\n",
      "|  {4, 11}|2022-08-07T04:21:...|            {6, 6.5}|     4|         11|       6|              6.5|\n",
      "|  {7, 19}|2022-08-07T04:21:...|            {9, 6.5}|     7|         19|       9|              6.5|\n",
      "|  {4, 19}|2022-08-07T04:21:...|           {10, 5.1}|     4|         19|      10|              5.1|\n",
      "|  {5, 18}|2022-08-07T04:21:...|            {3, 0.8}|     5|         18|       3|              0.8|\n",
      "|  {9, 16}|2022-08-07T04:21:...|{7, 9.20000000000...|     9|         16|       7|9.200000000000001|\n",
      "|  {8, 12}|2022-08-07T04:21:...|            {2, 6.3}|     8|         12|       2|              6.3|\n",
      "|  {8, 20}|2022-08-07T04:21:...|            {2, 0.2}|     8|         20|       2|              0.2|\n",
      "|  {5, 18}|2022-08-07T04:21:...|            {5, 2.5}|     5|         18|       5|              2.5|\n",
      "|  {5, 16}|2022-08-07T04:21:...|            {3, 6.5}|     5|         16|       3|              6.5|\n",
      "+---------+--------------------+--------------------+------+-----------+--------+-----------------+\n",
      "\n",
      "+--------------------+------+-----------+--------+-----------------+\n",
      "|            datetime|clicks|impressions|quantity|      total_price|\n",
      "+--------------------+------+-----------+--------+-----------------+\n",
      "|2022-08-07T04:21:...|     6|         13|       5|              6.4|\n",
      "|2022-08-07T04:21:...|     4|         11|       6|              6.5|\n",
      "|2022-08-07T04:21:...|     7|         19|       9|              6.5|\n",
      "|2022-08-07T04:21:...|     4|         19|      10|              5.1|\n",
      "|2022-08-07T04:21:...|     5|         18|       3|              0.8|\n",
      "|2022-08-07T04:21:...|     9|         16|       7|9.200000000000001|\n",
      "|2022-08-07T04:21:...|     8|         12|       2|              6.3|\n",
      "|2022-08-07T04:21:...|     8|         20|       2|              0.2|\n",
      "|2022-08-07T04:21:...|     5|         18|       5|              2.5|\n",
      "|2022-08-07T04:21:...|     5|         16|       3|              6.5|\n",
      "+--------------------+------+-----------+--------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(datetime,StringType,true),StructField(clicks,StringType,true),StructField(impressions,StringType,true),StructField(quantity,StringType,true),StructField(total_price,StringType,true)))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show()\n",
    "\n",
    "#a = df.select(\"analytics.clicks\")\n",
    "\n",
    "df = df.withColumn('clicks',df.analytics.clicks)\n",
    "#df.show()\n",
    "\n",
    "df = df.withColumn('impressions',df.analytics.impressions)\n",
    "\n",
    "df = df.withColumn('quantity',df.sales.quantity)\n",
    "df = df.withColumn('total_price',df.sales.total_price)\n",
    "\n",
    "df = df.drop(\"analytics\",\"sales\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "#a =  df.withColumn(\"Click\",col(\"analytics\").cast('string'))\n",
    "\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "json_df = spark.readStream.option(\"maxFilesPerTrigger\",5).schema(sc).format('json').option('header','true').load('/home/ubuntu/Spark/Spark_Work/python_file/data/json_files')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#json_df.printSchema()\n",
    "\n",
    "json_df.isStreaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = json_df\n",
    "\n",
    "\n",
    "df = df.withColumn('clicks',df.analytics.clicks)\n",
    "\n",
    "df = df.withColumn('impressions',df.analytics.impressions)\n",
    "\n",
    "df = df.withColumn('quantity',df.sales.quantity)\n",
    "df = df.withColumn('total_price',df.sales.total_price)\n",
    "\n",
    "df = df.drop(\"analytics\",\"sales\")\n",
    "\n",
    "#f_df = df.select('datetime','clicks','impressions','quantity','total_price')\n",
    "\n",
    "#f_df.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/10 12:33:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/10 23:39:20 ERROR MicroBatchExecution: Query [id = 94d71546-fe6c-4ee2-a6ff-6fe64613b280, runId = d84d55b3-785c-4976-afdf-dab48ced1962] terminated with error\n",
      "java.io.FileNotFoundException: File file:/home/ubuntu/Spark/Spark_Work/python_file/data/checkpoint/sources/0 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$addNewBatchByStream$2(HDFSMetadataLog.scala:173)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:171)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:116)\n",
      "\tat org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.add(CompactibleFileStreamLog.scala:168)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSourceLog.add(FileStreamSourceLog.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSource.fetchMaxOffset(FileStreamSource.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSource.latestOffset(FileStreamSource.scala:324)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:396)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:387)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:384)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:627)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:210)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "22/08/20 08:19:02 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 797606482 ms exceeds timeout 120000 ms\n",
      "22/08/20 08:19:02 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "final_df = df\n",
    "\n",
    "final_df = final_df.writeStream.outputMode(\"append\").format(\"csv\").option(\"path\", \"/home/ubuntu/Spark/Spark_Work/python_file/data/csv_files/\").option(\"checkpointLocation\", \"/home/ubuntu/Spark/Spark_Work/python_file/data/checkpoint\").option(\"header\", \"true\").start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/10 10:49:21 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "final_df = df\n",
    "\n",
    "final_df = final_df.writeStream.outputMode(\"append\").format(\"csv\").option(\"path\", \"/home/ubuntu/Spark/Spark_Work/python_file/data/csv_files/\").option(\"checkpointLocation\", \"/home/ubuntu/Spark/Spark_Work/python_file/data/checkpoint\").option(\"header\", \"true\").start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final_df.isStreaming\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/Spark/Spark_Work/Task_1.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ubuntu/Spark/Spark_Work/Task_1.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m spark\u001b[39m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
