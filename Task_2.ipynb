{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.streaming import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, LongType, DoubleType, FloatType, DataType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"Task_2\")\n",
    " .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"/home/ubuntu/Documents/Spark_Streaming/python_file/data/csv_files\"\n",
    "parquet_file_path = \"/home/ubuntu/Documents/Spark_Streaming/python_file/data/parquet_files/\"\n",
    "parquet_checkpoint_path = \"/home/ubuntu/Documents/Spark_Streaming/python_file/data/checkpoint_parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc = StructType([\n",
    "            StructField('datetime', TimestampType(), True),\n",
    "            StructField('clicks', IntegerType(), True),\n",
    "            StructField('impressions', IntegerType(), True),\n",
    "            StructField('quantity', IntegerType(), True),\n",
    "            StructField('total_price', DoubleType(), True)\n",
    "         ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading the data in stream from the CSV folder\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "csv_df = spark.readStream.option(\"maxFilesPerTrigger\",5).schema(sc).format('csv').option('header','true').load(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_df.printSchema()\n",
    "\n",
    "csv_df.isStreaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The main transformation where we are aggregating the data based on a window and watermark. This watermark will wait for the late arriving data is any.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "windows_df = csv_df.withWatermark('datetime','10 seconds').groupBy(window('datetime','30 seconds')).agg(sum('clicks').alias(\"Clicks\"),sum('impressions').alias(\"Impressions\"),sum('quantity').alias(\"Quantity\"),sum('total_price').alias(\"Total_price\"))\n",
    "\n",
    "df = windows_df\n",
    "\n",
    "df = df.withColumn('Start',df.window.start)\n",
    "\n",
    "df = df.withColumn('End',df.window.end)\n",
    "\n",
    "df = df.drop('window')\n",
    "\n",
    "\n",
    "f_df = df.select('Clicks','Impressions','Quantity','Total_price','Start','End')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Used foreachBatch because in a stream we can have empty data arriving and to tackle that used foreachBatch so that can use the functions of spark batch \n",
    "and based on that checking if we have data or not and write if have any data in parquet format else skipping that part.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def writer(f_df,batch):\n",
    "    if(f_df.count() <= 0):\n",
    "        print(\"Empty Data\")\n",
    "        print(f_df.count())\n",
    "    else:\n",
    "        print(\"Not Empty\")\n",
    "        f_df.show()\n",
    "        f_df.coalesce(1).write.format('csv').mode('append').option(\"path\", parquet_file_path).option('header','true').save()\n",
    "\n",
    "f_df.writeStream.outputMode('append').foreachBatch(writer).start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "For debugging or checking if we getting the correct count we can output our data to console and see data over here.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "final_df = csv_df\n",
    "\n",
    "final_df.writeStream.outputMode(\"append\").format(\"console\").option(\"path\", parquet_file_path).option(\"checkpointLocation\", \"/home/ubuntu/Spark/Spark_Work/python_file/data/checkpoint_parquet\").start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
